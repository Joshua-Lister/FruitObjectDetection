{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "domestic-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from torchvision import models\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "awful-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['background', 'orange', 'apple', 'banana']\n",
    "label2targets = {l: t for t, l in enumerate(labels)}\n",
    "targets2label = {t: l for l, t in label2targets.items()}\n",
    "num_classes = len(targets2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "german-poland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available!\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False  ##uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. -\n",
    "    torch.backends.cudnn.enabled   = False\n",
    "\n",
    "    return True\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
    "    print(\"Cuda installed! Running on GPU!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"No GPU available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "informative-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(xml_file, scale=1):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    bounding_boxes = []\n",
    "    labels = []\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = int(bbox.find('xmin').text) / scale\n",
    "        ymin = int(bbox.find('ymin').text) / scale\n",
    "        xmax = int(bbox.find('xmax').text) / scale\n",
    "        ymax = int(bbox.find('ymax').text) / scale\n",
    "        bounding_boxes.append((xmin, ymin, xmax, ymax))\n",
    "        labels.append(obj.find('name').text)\n",
    "\n",
    "    return bounding_boxes, labels\n",
    "\n",
    "def load_dataset(folder):\n",
    "    output_data = {}\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.xml'):\n",
    "            xml_path = os.path.join(folder, filename)\n",
    "            image_name = os.path.splitext(filename)[0] + '.jpg'\n",
    "            image_path = os.path.join(folder, image_name)\n",
    "            img = Image.open(image_path)\n",
    "            if os.path.exists(image_path):\n",
    "                bounding_boxes, name = parse_xml(xml_path)\n",
    "                output_data[image_name] = {'image': img, 'bounding_boxes': bounding_boxes, 'label': name}\n",
    "            else:\n",
    "                print(f\"Image file {image_name} not found for XML file {filename}\")\n",
    "\n",
    "    df = pd.DataFrame(output_data)\n",
    "    df = df.T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "missing-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitDataSet(Dataset):\n",
    "    def __init__(self, root_dir, transforms=None, resize_params=(224, 224)):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.img_paths = sorted(glob(self.root_dir + '/*.jpg'))\n",
    "        self.xml_paths = sorted(glob(self.root_dir + '/*.xml'))\n",
    "        self.resize_params = resize_params\n",
    "    \n",
    "    def parse_xml(self, xml_file, dims):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        bounding_boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = (int(bbox.find('xmin').text) / dims[1]) * self.resize_params[1]\n",
    "            ymin = (int(bbox.find('ymin').text) / dims[0]) * self.resize_params[0]\n",
    "            xmax = (int(bbox.find('xmax').text) / dims[1]) * self.resize_params[1]\n",
    "            ymax = (int(bbox.find('ymax').text) / dims[0]) * self.resize_params[0]\n",
    "            bounding_boxes.append((xmin, ymin, xmax, ymax))\n",
    "            labels.append(obj.find('name').text)\n",
    "\n",
    "        return bounding_boxes, labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        xml_path = self.xml_paths[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        dims = img.shape[:2]\n",
    "        img = cv2.resize(img, self.resize_params, cv2.INTER_AREA)\n",
    "        img /= 255.0\n",
    "        \n",
    "    \n",
    "        bounding_boxes, labels = self.parse_xml(xml_path, dims)\n",
    "        bounding_boxes = torch.tensor(bounding_boxes).float()\n",
    "        area = torch.as_tensor((bounding_boxes[:, 3] - bounding_boxes[:, 1]) * (bounding_boxes[:, 2] - bounding_boxes[:, 0]))\n",
    "          # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros(( bounding_boxes.shape[0],), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target['labels'] = torch.tensor([label2targets[label] for label in labels]).long()\n",
    "        target['boxes'] = bounding_boxes\n",
    "        target['area'] = area\n",
    "        #target['iscrowd'] = iscrowd\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        \n",
    "        if self.transforms:\n",
    "            sample = {\n",
    "                \"image\": img,\n",
    "                \"bboxes\": target[\"boxes\"],\n",
    "                'labels': labels\n",
    "            }\n",
    "            \n",
    "        return torch.tensor(img).permute(2, 0 ,1), target\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spread-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "floppy-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FruitDataSet(root_dir='train')\n",
    "test_ds = FruitDataSet(root_dir='test')\n",
    "#print('length of dataset = ', len(train_ds), '\\n')\n",
    "img, target = train_ds[78]\n",
    "#print(img.shape, \"\\n\", target)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn =  collate_fn)\n",
    "test_dl = DataLoader(test_ds, batch_size=4, shuffle=False,  collate_fn =  collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "remarkable-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'labels': tensor([2, 2]), 'boxes': tensor([[109.2599,  31.0435, 199.3394, 206.3478],\n",
      "        [ 27.0581,  10.9565, 115.7676, 164.3478]]), 'area': tensor([15791.3291, 13607.2617]), 'image_id': tensor([67])}, {'labels': tensor([2]), 'boxes': tensor([[ 18.6667,  31.3600, 205.3333, 221.7600]]), 'area': tensor([35541.3320]), 'image_id': tensor([20])}, {'labels': tensor([1, 3]), 'boxes': tensor([[ 39.4800,  45.8653, 139.7200, 151.7992],\n",
      "        [ 21.5600,  39.3554, 188.7200, 174.5839]]), 'area': tensor([10618.8203, 22604.8008]), 'image_id': tensor([166])}, {'labels': tensor([1]), 'boxes': tensor([[  6.2720,   4.7158, 124.5440, 165.0526]]), 'area': tensor([18963.3574]), 'image_id': tensor([235])})\n"
     ]
    }
   ],
   "source": [
    "images, targets = next(iter(train_dl))\n",
    "images = list(image.to(device) for image in images)\n",
    "print(targets)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acceptable-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_bbox(img, target):\n",
    "    # plot the image and bboxes\n",
    "    # Bounding boxes are defined as follows: x-min y-min width height\n",
    "    fig, a = plt.subplots(1,1)\n",
    "    fig.set_size_inches(6,6)\n",
    "    a.imshow(img)\n",
    "    for box in (target['bounding_boxes']):\n",
    "        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
    "        rect = patches.Rectangle((x, y),\n",
    "                                 width, height,\n",
    "                                 linewidth = 2,\n",
    "                                 edgecolor = 'r',\n",
    "                                 facecolor = 'none')\n",
    "\n",
    "        # Draw the bounding box on top of the image\n",
    "        a.add_patch(rect)\n",
    "    plt.show()\n",
    "    \n",
    "# plotting the image with bboxes. Feel free to change the index\n",
    "#img, target = test_ds[20]\n",
    "#plot_img_bbox(img, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-inspection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "prostate-boating",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/crasious/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-4-2 Python-3.7.10 torch-1.13.1+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "plastic-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exposed-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crasious/anaconda3/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/home/crasious/anaconda3/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 4)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "communist-syntax",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_classifier': tensor(1.6202, grad_fn=<NllLossBackward0>),\n",
       " 'loss_box_reg': tensor(0.2383, grad_fn=<DivBackward0>),\n",
       " 'loss_objectness': tensor(0.0059, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       " 'loss_rpn_box_reg': tensor(0.0049, grad_fn=<DivBackward0>)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(images, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-glance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1 loss: 1.9877402782440186\n",
      "Iteration #2 loss: 0.7592624425888062\n",
      "Iteration #3 loss: 0.8154124617576599\n",
      "Iteration #4 loss: 0.6115449666976929\n",
      "Iteration #5 loss: 1.065006971359253\n",
      "Iteration #6 loss: 0.714165449142456\n",
      "Iteration #7 loss: 0.6699426174163818\n",
      "Iteration #8 loss: 0.33624786138534546\n",
      "Iteration #9 loss: 0.4830794930458069\n",
      "Iteration #10 loss: 0.28032469749450684\n",
      "Iteration #11 loss: 0.29673856496810913\n",
      "Iteration #12 loss: 0.26447951793670654\n",
      "Iteration #13 loss: 0.47230643033981323\n",
      "Iteration #14 loss: 0.44779759645462036\n",
      "Iteration #15 loss: 0.3303823471069336\n",
      "Iteration #16 loss: 0.36142227053642273\n",
      "Iteration #17 loss: 0.2955268621444702\n",
      "Iteration #18 loss: 0.5276088118553162\n",
      "Iteration #19 loss: 0.5749486684799194\n",
      "Iteration #20 loss: 0.3386731743812561\n",
      "Iteration #21 loss: 0.2983100414276123\n",
      "Iteration #22 loss: 0.39120984077453613\n",
      "Iteration #23 loss: 0.18498551845550537\n",
      "Iteration #24 loss: 0.4596691131591797\n",
      "Iteration #25 loss: 0.35394197702407837\n",
      "Iteration #26 loss: 0.14453637599945068\n",
      "Iteration #27 loss: 0.19254206120967865\n",
      "Iteration #28 loss: 0.36956116557121277\n",
      "Iteration #29 loss: 0.43048012256622314\n",
      "Iteration #30 loss: 0.5002913475036621\n",
      "Iteration #31 loss: 1.0691128969192505\n",
      "Iteration #32 loss: 0.15194576978683472\n",
      "Iteration #33 loss: 0.386518269777298\n",
      "Iteration #34 loss: 0.19791916012763977\n",
      "Iteration #35 loss: 0.1922139972448349\n",
      "Iteration #36 loss: 0.3498891294002533\n",
      "Iteration #37 loss: 0.3128054440021515\n",
      "Iteration #38 loss: 0.5997903347015381\n",
      "Iteration #39 loss: 0.32080531120300293\n",
      "Iteration #40 loss: 0.2856040894985199\n",
      "Iteration #41 loss: 0.19799941778182983\n",
      "Iteration #42 loss: 0.3059529960155487\n",
      "Iteration #43 loss: 0.3799942135810852\n",
      "Iteration #44 loss: 0.5397856831550598\n",
      "Iteration #45 loss: 0.3053496778011322\n",
      "Iteration #46 loss: 0.32419195771217346\n",
      "Iteration #47 loss: 0.49702781438827515\n",
      "Iteration #48 loss: 0.2326136976480484\n",
      "Iteration #49 loss: 0.2196815311908722\n",
      "Iteration #50 loss: 0.25863271951675415\n",
      "Iteration #51 loss: 0.3581213653087616\n",
      "Iteration #52 loss: 0.2257230430841446\n",
      "Iteration #53 loss: 0.23764951527118683\n",
      "Iteration #54 loss: 0.13065128028392792\n",
      "Iteration #55 loss: 0.1919713318347931\n",
      "Iteration #56 loss: 0.09761679172515869\n",
      "Iteration #57 loss: 0.33953857421875\n",
      "Iteration #58 loss: 0.11247318238019943\n",
      "Iteration #59 loss: 0.2377176582813263\n",
      "Iteration #60 loss: 0.31870678067207336\n",
      "Iteration #61 loss: 0.12840589880943298\n",
      "Iteration #62 loss: 0.256113737821579\n",
      "Iteration #63 loss: 0.10275586694478989\n",
      "Iteration #64 loss: 0.23455922305583954\n",
      "Iteration #65 loss: 0.2598319947719574\n",
      "Iteration #66 loss: 0.29007503390312195\n",
      "Iteration #67 loss: 0.28413909673690796\n",
      "Iteration #68 loss: 0.19603998959064484\n",
      "Iteration #69 loss: 0.4132162928581238\n",
      "Iteration #70 loss: 0.3129781186580658\n",
      "Iteration #71 loss: 0.1382303088903427\n"
     ]
    }
   ],
   "source": [
    "itr = 1\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    for images, targets in train_dl:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        #print(targets)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "       \n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        #loss_hist.send(loss_value)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "        itr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image):\n",
    "    # Open image\n",
    "    \n",
    "    # Check image dimensions\n",
    "    width, height = image.size\n",
    "    print(width, height)\n",
    "    # If image dimensions are already (224, 224), return the original image\n",
    "    if width == 224 and height == 224:\n",
    "        return image\n",
    "    \n",
    "    # Resize image\n",
    "    resized_image = image.resize((224, 224))\n",
    "    \n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-harbor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
